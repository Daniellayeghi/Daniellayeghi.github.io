<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Notes on robotics and control.</title>
<meta name="description" content="Daniel's blog" />
<style type="text/css">
.post-dates{display:inline;padding-right:10px}
.post-desc{font-style:italic}
.post-footer{font-style:italic}
body{margin:40px auto;max-width:800px;line-height:1.6;font-size:18px;color:#444;padding:0 10px}
h1,h2,h3{line-height:1.2}
</style>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, 
        TeX: {equationNumbers: {autoNumber: "AMS"}}
    });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<h2>Combining Path Integral Control With Weighted DDP</h2>
<div class="post-desc"> 
25 Feb 2021
</div>

<div class="post-content">

<p>
This post explores the idea of combining DDP with path integral conrol namely the information theoretic formulation. The intuition behind this idea is to leverage the advantages of each method.
For example path inegral methods do not requrie convex objective functions and can approximate complex objectives at the expense large sampling and noisy control. On the other hand DDP as a form of trajectory optimisation is inherently deterministic and can solve well conditioned problems very quicky. However in the case where quadratic approximations are not accurate it suffers from converging to poor local optimums and in the worst case instability.
</p>

<h3>Reconstructing the Objective</h3>
To combine DDP with PI we can reformulated the objective function as such:
$$
\underset{u}{min}[q(x) + E_{x^{'}\sim u}[log\frac{u(x'|x)}{p(x'|x)}]+kE_{x^{'}\sim u}[log\frac{u(x'|x)}{ddp(x'|x)}]+E_{x^{'}\sim u}[v(x')]]
$$

The intuition behind this objective follows the bellman principle of optimiality with the additions of obeying the true dynamics of the system $p(x'|x)$ while staying close to the distiribution of the optimal controls
obtaind by DDP $ddp(x'|x)$ with some importance defined by $k$. 

<p>
We can combine all expectations:

$$
E_{x^{'}\sim u}[log\frac{u(x'|x)}{p(x'|x)}+klog\frac{u(x'|x)}{ddp(x'|x)}+v(x')]\\ 
=E_{x^{'}\sim u(.|x)}[log(\frac{u(x^{'}|x)^{k+1}}{p(x^{'}|x)ddp(x^{'}|x)^{k}exp(-v(x'))})]
$$

The above expression can be thought of as the KL divergence between two unnormalised distributions.Therefore replacing $exp(-v(x'))$ with $z(x')$ and multiplying and dividing the numerator and the denominator by their respective normalisation constants leads to:

$$
E_{x^{'}\sim u(.|x)}[log(\frac{u(x^{'}|x)^{k+1}}{p(x^{'}|x)ddp(x^{'}|x)^{k}z(x')})] = E_{x^{'}\sim u(.|x)}[log(\frac{u(x^{'}|x)^{k+1}f(x)/f(x)}{p(x^{'}|x)ddp(x^{'}|x)^{k}z(x)g(x)/g(x))})]
$$

Computing the new controls leads to:

$$
q(x) -log(g(x)) + \underset{u}{min}[E_{x^{'}\sim u(.|x)}[log(\frac{u(x^{'}|x)^{k+1}f(x)/f(x)}{p(x^{'}|x)ddp(x^{'}|x)^{k}z(x)/g(x))})]]
$$

The minimum is achieved when:

$$
u^*(x^{'}|x) = (p(x^{'}|x)ddp(x^{'}|x)^{k}z(x)/g(x)))^{\frac{1}{k+1}}
$$
</p> 

<p>
Considering the above likelihoods across full trajectories $V = [u_1, u_2, u_3, ... u_N]$ and replacing $u^*$ with trajectory distribution $q^*(V)$ we can then follow the information theoretic derevation for path integrals and the above can be written as probability distributions as:

$$q^*(V) = (\frac{1}{\eta}exp(\frac{1}{\lambda}S(V))p(V)ddp(v)^{k})^{\frac{1}{k+1}}$$

As a result the optimal control strategy is to minimise the divergence between the optimal $Q^*$ and the control $Q$ distributions $U^* = argmin_u KL(Q^*|| Q)$:
</p>

<h3> Modified Importance Sampling </h3>

<p>
Followoing the derervation from the information theoroetic approach we know that the minimisation leads to the optimal controls being the expecation of $u_s$ under the optimal distribution $Q^*$. Since this distribution is not known we will need to resort to importance sampling. However in this formulation the importance sampling is biased towards the joint distribution $q(V)ddp(V)$. As a result the importance sampling can be written as:

$$u^* = \int q(V)\underbrace{\frac{q^*(V)}{p(V)^{\frac{1}{k+1}}ddp(V)^{\frac{k}{k+1}}}\frac{p(V)^{\frac{1}{k+1}}ddp(V)^{\frac{k}{k+1}}}{q(V)}}_{w(v)} v_t dV$$

Assuming the each distribution as Gaussian with the following form:

$$
p(V) = \prod_{t=0}^{T-1}Z^{-1}\exp(\frac{1}{2}v_t^{T}\Sigma^{-1}v_t) = Z^{-1}\exp(-\frac{1}{2}\sum_{t=0}^{T_1}v_t^{T}\Sigma^{-1}v_t) \\
q(V) = \prod_{t=0}^{T-1}Z^{-1}\exp(\frac{1}{2}(v_t - u_t)^T\Sigma^{-1}(v_t - u_t)) = Z^{-1}\exp(-\frac{1}{2}\sum_{t=0}^{T_1}(v_t - u_t)^T\Sigma^{-1}(v_t - u_t)) \\
ddp(U) = \prod_{t=0}^{T-1}Z^{-1}_{ddp}\exp(\frac{1}{2}(v_t - u_{ddp})^T\Sigma^{-1}_{ddp}(v_t - u_{ddp})) = Z^{-1}_{ddp}\exp(-\frac{1}{2}\sum_{t=0}^{T_1}(v_t - u_{ddp})^T\Sigma^{-1}_{ddp}(v_t - u_{ddp}))
$$

Solving for the weights:

$$
w(v) = \frac{1}{\eta z z_{ddp}}\exp(-\frac{1}{2}\frac{1}{\lambda}S(V)\sum_{t=0}^{T_1}\underbrace{\frac{k}{k+1}({v_t}^T\Sigma_{ddp}^{-1}v_t-2{v_t}^T\Sigma_{ddp}^{-1}u_{ddp}+{v_{ddp}}^T\Sigma_{ddp}^{-1}u_{ddp}-{v_t}^T\Sigma^{-1}v_t)}_{\text{DDP Bias}} + \underbrace{2{v_t}^T\Sigma^{-1}u_t-{u_t}^T\Sigma^{-1}u_t}_{\text{Path Integral Bias}})
$$


The normalisation factor $\frac{1}{\eta z z_{ddp}}$ can also be approximated by monte carlo sampling. As a result it is obvious that the optimal control with the bias of $k = 0$ is equivalent to the path integral formulation and with factors $k > 0$ the control distribution is biased towards DDP solution.
</p>


</div>


<div class="post-footer">
<a href="/blog/">Home</a>
</div>

</body>
</html>

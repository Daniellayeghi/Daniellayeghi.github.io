<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Neural Stochastic Optimal Control</title>
<meta name="description" content="Thesis segment blog post"/>
<link rel="stylesheet" href="../css/site-common.css" type="text/css"/>
<link rel="stylesheet" href="../css/blog-post.css" type="text/css"/>
<script>
window.MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['\\[', '\\]']],
    macros: {
      vec: ['\\mathbf{#1}', 1]
    }
  },
  svg: { fontCache: 'global' }
};
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>
  <main role="main">
    <div class="container">
      <p class="back"><a href="/blog/">&larr; Back to blog index</a></p>
      <header class="page-header">
        <h1>Neural Stochastic Optimal Control</h1>
      </header>

      <section class="section">
        
        <h3>Problem statement</h3>
        <p>Numerical optimal control solvers efficiently compute locally optimal trajectories given smooth and differentiable objective functions. Conversely, RL approximates general policies while leveraging sampling to impose minimal constraints on the objective.</p>
        <p><strong>Aim.</strong> Formulate a global method that leverages sampling to enable <strong>flexibility</strong> inspired by RL while maintaining <strong>efficiency</strong> and <strong>generalisation</strong>.</p>

        <h3>Continuous stochastic Bellman equation</h3>
        <p>In this case, we use the stochastic HJB equation.</p>
        <div class="equation">
          \[
          -\frac{\partial v\left(\vec x_t,t\right)}{\partial t}=\min_{\vec u_t}\left[\ell\left(\vec x_t,\vec u_t\right)+\nabla_{\vec x_t}v\left(\vec x_t,t\right)\vec f\left(\vec x_t,\vec u_t\right)+\frac{1}{2}\operatorname{trace}\left(\nabla^2_{\vec x_t\vec x_t}v\left(\vec x_t,t\right)\vec\Sigma\left(\vec x_t\right)\right)\right]
          \]
        </div>
        <p><strong>Optimal controls.</strong> Assuming affine control \(d\vec x_t=\left(\vec h\left(\vec x_t\right)+\vec g\left(\vec x_t\right)\vec u_t\right)\,dt+\vec B\left(\vec x_t\right)\,d\vec w_t\) and quadratic control cost, we can analytically solve for \(\vec u_t\):</p>
        <div class="equation">
          \[
          \vec u^*\left(\vec x_t\right)=-\left(\nabla^2_{\vec u_t}\ell_{\text{ctrl}}\left(\vec u_t\right)\right)^{-1}\left(\nabla_{\vec x_t}v\left(\vec x_t,t\right)\nabla_{\vec u_t}\vec f\left(\vec x_t,\vec u_t\right)\right)
          \]
        </div>
        <p>Our optimal control remains the same, and so does our choice of parameterisation: the value function.</p>

        <h3>Learning the value function</h3>
        <p>As in the deterministic case, we define a constraint on the rate of change of the value function.</p>
        <div class="equation">
          \[
          \begin{aligned}
          \mathbb E_{\mathcal Q}\left[\frac{dv\left(\vec x_t,t\right)}{dt}\right]&=-\ell\left(\vec x_t,\vec u_t^*\right)\\
          \mathbb E_{\mathcal Q}\left[v\left(\vec x_t,t\right)-v\left(\vec x_{t+\Delta t},t+\Delta t\right)\right]&=\ell\left(\vec x_t,\vec u_t^*\right)\Delta t\\
          v\left(\vec x_t,t\right)-\mathbb E_{\mathcal Q}\left[v\left(\vec x_{t+\Delta t},t+\Delta t\right)\right]&=\ell\left(\vec x_t,\vec u_t^*\right)\Delta t
          \end{aligned}
          \]
        </div>
        <p><strong>Intuition.</strong> Under policy \(\vec u_t^*\), the expected change in value decreases at a rate negative to cost.</p>

        <h3>TD(ish) view</h3>
        <p>Satisfying this constraint over horizon \(N\) can be viewed through a lens similar to TD.</p>
        <div class="equation">
          \[
          \begin{aligned}
          \mathbb E_{\mathcal Q}\left[v\left(\vec x_N,N\right)\right]-\mathbb E_{\mathcal Q}\left[\psi\left(\vec x_N\right)\right]&=0\\
          v\left(\vec x_{N-1},N-1\right)-\mathbb E_{\mathcal Q}\left[v\left(\vec x_N,N\right)\right]-\ell\left(\vec x_{N-1},\vec u^*_{N-1}\right)\Delta t&=0\\
          v\left(\vec x_{N-2},N-2\right)-\mathbb E_{\mathcal Q}\left[v\left(\vec x_{N-1},N-1\right)\right]-\ell\left(\vec x_{N-2},\vec u^*_{N-2}\right)\Delta t&=0\\
          &\vdots\\
          v\left(\vec x_0,0\right)-\mathbb E_{\mathcal Q}\left[v\left(\vec x_1,1\right)\right]-\ell\left(\vec x_0,\vec u^*_0\right)\Delta t&=0
          \end{aligned}
          \]
        </div>
        <p><strong>Note.</strong> We cannot simply sum costs backwards due to the expectation under \(\mathcal Q\).</p>

        <h3>Discrete algorithm</h3>
        <div class="algo-block">
          <pre><span class="algo-keyword">Initialize:</span> x(0)=x<sub>0</sub>, Δt, N=T/Δt, E<sub>Q</sub>[v(x<sub>N</sub>,N)] = E<sub>Q</sub>[ψ(x<sub>N</sub>)]
<span class="algo-keyword">For</span> i = 0 ... N−1:
  u<sub>i</sub> = −(∇²<sub>u_i</sub> l<sub>ctrl</sub>(u<sub>i</sub>))<sup>−1</sup> g(x<sub>i</sub>)<sup>T</sup> ∇<sub>x</sub> ṽ(x<sub>i</sub>, i; θ)<sup>T</sup>
  x<sub>i+1</sub> = x<sub>i</sub> + ( h(x<sub>i</sub>) + g(x<sub>i</sub>)u<sub>i</sub> ) Δt + B(x<sub>i</sub>) ε<sub>i</sub> √Δt
<span class="algo-keyword">Optimize:</span> min<sub>θ</sub> Σ<sub>i=0..N</sub> ( ṽ(x<sub>i</sub>, i, θ) − E<sub>Q</sub>[ṽ(x<sub>i+1</sub>, i+1, θ)] − l(x<sub>i</sub>, u<sub>i</sub><sup>*</sup>) Δt )<sup>2</sup></pre>
        </div>
        <p><strong>Intuition.</strong> Roll out trajectories under the current optimal policy, then fit the stochastic Bellman difference.</p>

        <h3>Results</h3>
        <div class="media-grid">
          <img src="value%20learning%20content/cartpole_balance_trajectory_cost_ieee_stochastic.png" alt="Stochastic cartpole balance trajectory cost"/>
          <img src="value%20learning%20content/cartpole_swingup_trajectory_cost_ieee_stochastic.png" alt="Stochastic cartpole swingup trajectory cost"/>
          <img src="value%20learning%20content/reacher_trajectory_cost_ieee_stochastic.png" alt="Stochastic reacher trajectory cost"/>
          <img src="value%20learning%20content/cartpole_balancing_rewards_ieee_stochastic.png" alt="Stochastic cartpole balancing rewards"/>
          <img src="value%20learning%20content/cartpole_swingup_rewards_ieee_stochastic.png" alt="Stochastic cartpole swingup rewards"/>
          <img src="value%20learning%20content/reahcer_rewards_ieee_stochastic.png" alt="Stochastic reacher rewards"/>
        </div>
        <p>Overview: significantly faster convergence and lower variance across random seeds, outperforming SAC and PPO by at least factors of 18 and 2, respectively.</p>

        <h3>Noise-driven smoothing (Obstacle avoidance with discontinuous objective)</h3>
        <p>Value approximation under stochastic dynamics and discontinuous objectives.</p>
        <div class="media-grid">
          <img src="value%20learning%20content/1stnoise.png" alt="Noise smoothing example 1"/>
          <img src="value%20learning%20content/3rdnoise.png" alt="Noise smoothing example 2"/>
          <img src="value%20learning%20content/4thnoise.png" alt="Noise smoothing example 3"/>
        </div>
        <p>Higher noise smooths the value function through the curvature term \(\operatorname{trace}\left(\nabla^2_{\vec x_t\vec x_t}v\left(\vec x_t,t\right)\vec\Sigma\left(\vec x_t\right)\right)\).</p>
        <p>As we increase the control noise particles take trajectories further from obstacles. This is confirmation that noise penalises value function curvature, improving robustness.<p>

        <h3>Conclusion</h3>
        <div class="conclusion-grid">
          <div class="conclusion-block">
            <h4>Contribution</h4>
            <ul>
              <li>Global policy.</li>
              <li>Flexibility to discontinuity via noise.</li>
              <li>Better efficiency.</li>
            </ul>
          </div>
          <div class="conclusion-block">
            <h4>Caveats</h4>
            <ul>
              <li>Requires full dynamics information.</li>
              <li>SDE rollouts are considerably more memory-intensive.</li>
            </ul>
          </div>
        </div>
      </section>
    </div>
  </main>
</body>
</html>
